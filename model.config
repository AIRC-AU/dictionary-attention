embed_dim           = 512   # dimensions of embedding sublayers
ff_dim              = 2048  # dimensions of feed-forward sublayers
num_heads           = 8     # number of parallel attention heads
num_layers          = 6     # number of encoder/decoder layers
dropout             = 0.3   # dropout for feed-forward/attention sublayers
max-epochs          = 50    # maximum number of epochs, halt training
lr                  = 3e-4  # learning rate (step size of optimizer)
patience            = 3     # number of epochs without improvement
decay-factor        = 0.8   # if patience reached, lr *= decay-factor
min-lr              = 5e-5  # minimum learning rate, halt training
label-smoothing     = 0.1   # label smoothing (regularization technique)
batch-size          = -1    # number of tokens (source + target) per batch;
                            # if batch-size = -1, fill available GPU memory
                            # using max-length and binary search
max-length          = 256   # maximum sentence length (if batch-size = -1)
beam-width          = 4     # beam search and length normalization
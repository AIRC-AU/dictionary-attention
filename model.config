d_model             = 512   # dimensions of embedding layers
d_ff                = 2048  # dimensions of feed-forward layers
num_heads           = 8     # number of parallel attention heads
num_layers          = 6     # number of encoder/decoder layers
dropout             = 0.3   # dropout rate for each sublayer
epoch-size          = 1000  # number of iterations considered one epoch
max-epochs          = 50    # maximum number of epochs, halt training
lr                  = 3e-4  # learning rate (step size of optimizer)
patience            = 3     # number of epochs without improvement
min-delta           = 0.0   # minimum change to qualify as an improvement
lr-eta              = 0.8   # if patience reached, lr *= lr-eta
min-lr              = 5e-5  # minimum learning rate, halt training
label-smoothing     = 0.1   # label smoothing (regularization technique)
batch-size          = -1    # number of tokens (source + target) per batch;
                            # if batch-size = -1, fill available GPU memory
                            # using max-length and binary search
max-length          = 256   # maximum sentence length (if batch-size = -1)
beam-width          = 4     # beam search and length normalization